{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interim-current",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, time, gc, random\n",
    "import datatable as dt\n",
    "import numpy as np\n",
    "import janestreet\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "competitive-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机种子\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-religion",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inappropriate-whale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Filling...\n",
      "Converting...\n",
      "CPU times: user 17min 21s, sys: 25.9 s, total: 17min 47s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Loading...')\n",
    "train = dt.fread('/kaggle/working/input/train.csv').to_pandas()\n",
    "print('Filling...')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "f_mean = train[features[1:]].mean()\n",
    "train = train.loc[train.weight > 0].reset_index(drop = True)\n",
    "train[features[1:]] = train[features[1:]].fillna(f_mean)\n",
    "train['action'] = (train['resp'] > 0).astype('int')\n",
    "print('Converting...')\n",
    "f_mean = f_mean.values\n",
    "np.save('f_mean.npy', f_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tough-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling\n"
     ]
    }
   ],
   "source": [
    "print('Modeling')\n",
    "# 4 层Dense # 参数为啥这么选？\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = nn.BatchNorm1d(len(features))\n",
    "        self.dropout0 = nn.Dropout(0.10143786981358652)\n",
    "\n",
    "        hidden_size = 256\n",
    "        self.dense1 = nn.Linear(len(features), 384)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(384)\n",
    "        self.dropout1 = nn.Dropout(0.19720339053599725)\n",
    "\n",
    "        self.dense2 = nn.Linear(384, 896)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(896)\n",
    "        self.dropout2 = nn.Dropout(0.2703017847244654)\n",
    "\n",
    "        self.dense3 = nn.Linear(896, 896)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(896)\n",
    "        self.dropout3 = nn.Dropout(0.23148340929571917)\n",
    "\n",
    "        self.dense4 = nn.Linear(896, 394)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(394)\n",
    "        self.dropout4 = nn.Dropout(0.2357768967777311)\n",
    "\n",
    "        self.dense5 = nn.Linear(394, 1)\n",
    "\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "        self.PReLU = nn.PReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = nn.GELU()\n",
    "        self.RReLU = nn.RReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.dense4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limited-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDataset:\n",
    "    def __init__(self, df):\n",
    "        self.features = df[features].values\n",
    "        self.label = (df['resp'] > 0).astype('int').values.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "important-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features = data['features'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        final_loss += loss.item()\n",
    "    final_loss /= len(dataloader)\n",
    "    return final_loss\n",
    "\n",
    "# 推理\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for data in dataloader:\n",
    "        features = data['features'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "threaded-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alike-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score: #  + self.delta\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            # ema.apply_shadow()\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            # ema.restore()\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hawaiian-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_score_bincount(date, weight, resp, action):\n",
    "    count_i = len(np.unique(date))\n",
    "    Pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n",
    "    u = np.clip(t, 0, 6) * np.sum(Pi)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "juvenile-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD0 EPOCH:  0, train_loss:0.69331, u_score:1944.61904, auc:0.53405, logloss:0.69163, time: 0.36min\n",
      "Validation score improved (-inf --> 0.5340498877324772). Saving model!\n",
      "FOLD0 EPOCH:  1, train_loss:0.69091, u_score:1585.49528, auc:0.53132, logloss:0.69162, time: 0.62min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD0 EPOCH:  2, train_loss:0.69025, u_score:1998.73372, auc:0.53089, logloss:0.69216, time: 0.86min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "FOLD0 EPOCH:  3, train_loss:0.68956, u_score:1575.78973, auc:0.53250, logloss:0.69202, time: 1.11min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "FOLD1 EPOCH:  0, train_loss:0.69357, u_score:198.16977, auc:0.52507, logloss:0.69251, time: 1.40min\n",
      "Validation score improved (-inf --> 0.5250680495389416). Saving model!\n",
      "FOLD1 EPOCH:  1, train_loss:0.69066, u_score:504.07485, auc:0.52809, logloss:0.69258, time: 1.65min\n",
      "Validation score improved (0.5250680495389416 --> 0.5280923184354895). Saving model!\n",
      "FOLD1 EPOCH:  2, train_loss:0.69005, u_score:572.39166, auc:0.52746, logloss:0.69293, time: 1.90min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD1 EPOCH:  3, train_loss:0.68934, u_score:1046.47538, auc:0.53071, logloss:0.69274, time: 2.16min\n",
      "Validation score improved (0.5280923184354895 --> 0.5307121262531885). Saving model!\n",
      "FOLD1 EPOCH:  4, train_loss:0.68864, u_score:679.27451, auc:0.52864, logloss:0.69343, time: 2.42min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD1 EPOCH:  5, train_loss:0.68793, u_score:1080.50943, auc:0.53188, logloss:0.69310, time: 2.67min\n",
      "Validation score improved (0.5307121262531885 --> 0.5318763121425799). Saving model!\n",
      "FOLD1 EPOCH:  6, train_loss:0.68704, u_score:1142.61772, auc:0.53216, logloss:0.69381, time: 2.92min\n",
      "Validation score improved (0.5318763121425799 --> 0.5321555267190771). Saving model!\n",
      "FOLD1 EPOCH:  7, train_loss:0.68617, u_score:1704.93895, auc:0.53258, logloss:0.69291, time: 3.18min\n",
      "Validation score improved (0.5321555267190771 --> 0.532576483080852). Saving model!\n",
      "FOLD1 EPOCH:  8, train_loss:0.68526, u_score:1098.10958, auc:0.53142, logloss:0.69455, time: 3.43min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD1 EPOCH:  9, train_loss:0.68444, u_score:1914.29247, auc:0.53231, logloss:0.69550, time: 3.67min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "FOLD2 EPOCH:  0, train_loss:0.69388, u_score:2577.18834, auc:0.52941, logloss:0.69185, time: 3.96min\n",
      "Validation score improved (-inf --> 0.52941442278266). Saving model!\n",
      "FOLD2 EPOCH:  1, train_loss:0.69089, u_score:3841.91883, auc:0.53062, logloss:0.69229, time: 4.22min\n",
      "Validation score improved (0.52941442278266 --> 0.5306184927029803). Saving model!\n",
      "FOLD2 EPOCH:  2, train_loss:0.69000, u_score:3670.54429, auc:0.53395, logloss:0.69207, time: 4.48min\n",
      "Validation score improved (0.5306184927029803 --> 0.5339530167190937). Saving model!\n",
      "FOLD2 EPOCH:  3, train_loss:0.68948, u_score:3069.76807, auc:0.53473, logloss:0.69227, time: 4.73min\n",
      "Validation score improved (0.5339530167190937 --> 0.5347312377907208). Saving model!\n",
      "FOLD2 EPOCH:  4, train_loss:0.68863, u_score:3264.69298, auc:0.53166, logloss:0.69334, time: 4.99min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD2 EPOCH:  5, train_loss:0.68794, u_score:2516.34822, auc:0.53243, logloss:0.69358, time: 5.25min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "FOLD2 EPOCH:  6, train_loss:0.68717, u_score:2651.71847, auc:0.53168, logloss:0.69315, time: 5.54min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "FOLD3 EPOCH:  0, train_loss:0.69392, u_score:484.07322, auc:0.52668, logloss:0.69205, time: 5.83min\n",
      "Validation score improved (-inf --> 0.5266837199183272). Saving model!\n",
      "FOLD3 EPOCH:  1, train_loss:0.69072, u_score:326.68236, auc:0.52621, logloss:0.69279, time: 6.08min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD3 EPOCH:  2, train_loss:0.68995, u_score:392.68584, auc:0.52453, logloss:0.69316, time: 6.33min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "FOLD3 EPOCH:  3, train_loss:0.68906, u_score:117.60656, auc:0.52481, logloss:0.69277, time: 6.58min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "FOLD4 EPOCH:  0, train_loss:0.69372, u_score:2031.13398, auc:0.53302, logloss:0.69128, time: 6.88min\n",
      "Validation score improved (-inf --> 0.5330205149620717). Saving model!\n",
      "FOLD4 EPOCH:  1, train_loss:0.69080, u_score:1747.27806, auc:0.53165, logloss:0.69165, time: 7.15min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD4 EPOCH:  2, train_loss:0.68996, u_score:1706.26153, auc:0.53336, logloss:0.69188, time: 7.41min\n",
      "Validation score improved (0.5330205149620717 --> 0.5333637395917599). Saving model!\n",
      "FOLD4 EPOCH:  3, train_loss:0.68920, u_score:1482.30548, auc:0.52923, logloss:0.69350, time: 7.66min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "FOLD4 EPOCH:  4, train_loss:0.68830, u_score:1597.34058, auc:0.52959, logloss:0.69438, time: 7.94min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "FOLD4 EPOCH:  5, train_loss:0.68744, u_score:1699.27359, auc:0.52965, logloss:0.69360, time: 8.19min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "start_time = time.time()\n",
    "oof = np.zeros(len(train['action']))\n",
    "gkf = GroupKFold(n_splits = 5)\n",
    "for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n",
    "    train_set = MarketDataset(train.loc[tr])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    valid_set = MarketDataset(train.loc[te])\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "    \n",
    "    ckp_path = f'JSModel_{fold}.pth'\n",
    "    \n",
    "    es = EarlyStopping(patience=3, mode=\"max\")\n",
    "    for epoch in range(10):\n",
    "        train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n",
    "        valid_pred = inference_fn(model, valid_loader, device)\n",
    "        auc_score = roc_auc_score((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n",
    "        logloss_score = log_loss((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n",
    "        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n",
    "        u_score = utility_score_bincount(date=train.loc[te].date.values, weight=train.loc[te].weight.values, resp=train.loc[te].resp.values, action=valid_pred)\n",
    "\n",
    "        print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n",
    "              f\"time: {(time.time() - start_time) / 60:.2f}min\")\n",
    "        \n",
    "        es(auc_score, model, model_path=ckp_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "delayed-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    ckp_path = f'JSModel_{i}.pth'\n",
    "    model.load_state_dict(torch.load(ckp_path))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "widespread-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = np.load('./f_mean.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-regular",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "solar-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = janestreet.make_env()\n",
    "env_iter = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-turkish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052357197896422e9ef03eab4d5c6402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt_th = 0.5\n",
    "for (test_df, pred_df) in tqdm(env_iter):\n",
    "    if test_df['weight'].item() > 0:\n",
    "        x_tt = test_df.loc[:, features].values\n",
    "        if np.isnan(x_tt[:, 1:].sum()):\n",
    "            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "        pred = 0.\n",
    "        \n",
    "        for i, clf in enumerate(models):\n",
    "            if i == 0:\n",
    "                pred = clf(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / len(models)\n",
    "            else:\n",
    "                pred += clf(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / len(models)\n",
    "        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "    env.predict(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
