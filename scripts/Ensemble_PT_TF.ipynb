{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-apparel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, time, gc, random\n",
    "import datatable as dt\n",
    "import numpy as np\n",
    "import janestreet\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "from numba import njit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-avenue",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "checked-cameroon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 36s, sys: 19.5 s, total: 5min 56s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = dt.fread('/kaggle/working/input/train.csv').to_pandas()\n",
    "# train = dt.fread('/kaggle/input/jane-street-market-prediction/train.csv').to_pandas()\n",
    "train = train.query('date > 85').reset_index(drop=True)\n",
    "train = train.loc[train.weight > 0].reset_index(drop = True)\n",
    "\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "f_mean = train[features[1:]].mean()\n",
    "train[features[1:]] = train[features[1:]].fillna(f_mean)\n",
    "f_mean = f_mean.values\n",
    "\n",
    "train['action'] = (train['resp'] > 0).astype('int')\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "steady-given",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.loc[:, features].values\n",
    "y_train = (train[resp_cols] > 0).astype(int).values\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-involvement",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valued-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MarketDataset:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.features = X_train\n",
    "        self.label = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_colunms, num_labels):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = nn.BatchNorm1d(num_colunms)\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense1 = nn.Linear(num_colunms, 384)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(384)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense2 = nn.Linear(384, 896)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(896)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense3 = nn.Linear(896, 896)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(896)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense4 = nn.Linear(896, 394)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(394)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense5 = nn.Linear(394, num_labels)\n",
    "\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "        self.PReLU = nn.PReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = nn.GELU()\n",
    "        self.RReLU = nn.RReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.dense4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = x * F.sigmoid(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features = data['features'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        final_loss += loss.item()\n",
    "    final_loss /= len(dataloader)\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "precise-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Time:14.46s, Loss 0.6913047307170928\n",
      "Epoch:1, Time:12.66s, Loss 0.6892876237009963\n",
      "Epoch:2, Time:13.65s, Loss 0.6887070592492819\n",
      "Epoch:3, Time:12.57s, Loss 0.6883050406662127\n",
      "Epoch:4, Time:12.36s, Loss 0.6879889594080547\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "num_colunms = len(features)\n",
    "num_labels = len(resp_cols)\n",
    "batch_size = 4096\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_set = MarketDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "pt_predictor = Model(num_colunms=num_colunms, num_labels=num_labels)\n",
    "pt_predictor.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(pt_predictor.parameters(), lr=learning_rate)\n",
    "loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_fn(pt_predictor, optimizer, None, loss_fn, train_loader, device)\n",
    "    end = time.time()\n",
    "    print('Epoch:{}, Time:{:.2f}s, Loss {}'.format(epoch, end - start, train_loss))\n",
    "    start = end\n",
    "torch.save(pt_predictor, 'pt_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-shipping",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verbal-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1111)\n",
    "def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "        \n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate), # RectifiedAdam Optimizer (known to be robust to the choice in learning rate)\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    ) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "catholic-colon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "384/384 - 4s - loss: 0.7170 - AUC: 0.5120\n",
      "Epoch 2/10\n",
      "384/384 - 5s - loss: 0.6941 - AUC: 0.5275\n",
      "Epoch 3/10\n",
      "384/384 - 5s - loss: 0.6913 - AUC: 0.5342\n",
      "Epoch 4/10\n",
      "384/384 - 4s - loss: 0.6904 - AUC: 0.5383\n",
      "Epoch 5/10\n",
      "384/384 - 4s - loss: 0.6899 - AUC: 0.5412\n",
      "Epoch 6/10\n",
      "384/384 - 5s - loss: 0.6897 - AUC: 0.5428\n",
      "Epoch 7/10\n",
      "384/384 - 5s - loss: 0.6895 - AUC: 0.5440\n",
      "Epoch 8/10\n",
      "384/384 - 5s - loss: 0.6893 - AUC: 0.5451\n",
      "Epoch 9/10\n",
      "384/384 - 5s - loss: 0.6890 - AUC: 0.5468\n",
      "Epoch 10/10\n",
      "384/384 - 5s - loss: 0.6888 - AUC: 0.5477\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 4096\n",
    "hidden_units = [160, 160, 160]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf_predictor = create_mlp(len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate)\n",
    "tf_predictor.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "tf_predictor.save('tf_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-disorder",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stuffed-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "import janestreet\n",
    "env = janestreet.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "explicit-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def fast_fillna(array, values):\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designing-brunswick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e176472d3af24cc69cf8e5bdefc519e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 30s, sys: 2.82 s, total: 3min 33s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "opt_th = 0.503\n",
    "tmp = np.zeros(len(features))\n",
    "pt_predictor.eval()\n",
    "for (test_df, prediction_df) in tqdm(iter_test):\n",
    "    if test_df['weight'].values[0] > 0:\n",
    "        x_tt = test_df.loc[:, features].values\n",
    "        x_tt[0, :] = fast_fillna(x_tt[0, :], tmp)\n",
    "        tmp = x_tt[0, :]\n",
    "        pt_preds = np.median(pt_predictor(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy())\n",
    "        tf_preds = np.median(tf_predictor(x_tt))\n",
    "        prediction_df[\"action\"].values[0] = int((0.6 * tf_preds + 0.4 * pt_preds) >= opt_th)\n",
    "    else:\n",
    "        prediction_df[\"action\"].values[0] = 0\n",
    "    env.predict(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-paper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
